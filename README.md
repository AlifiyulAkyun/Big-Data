# Big-Data
## Chapter 3


### Scala

**1. System Commands Output**
<table border="0">
 <tr>
  <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
   <td>https://github.com/AlifiyulAkyun/Big-Data/blob/19d8e266b64bfc11feee8d6762996fc985728dda/SystemCommandsOutput/SystemCommandsOutput.scala#L1-L3 </td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/SystemCommandsOutput/SystemCommandsOutput.png"></td>
 </tr>
</table><br>

**2. System Commands ReturnCode**
<table border="0">
 <tr>
    <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/SystemCommandsReturnCode/SystemCommandsReturnCode.scala#L1-L3 </td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/SystemCommandsReturnCode/SystemCommandsReturnCode.png"></td>
 </tr>
</table>

### Python
**1. Acccumulator**
<table border="0">
 <tr>
    <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/Accumulator/Accumulator.py#L1-L4 </td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/Accumulator/Accumulator.png"></td>
 </tr>
</table>
<b>2. BroadCast</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/BroadCast/BroadCast.py#L1-L2</td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/BroadCast/Broadcast.png"></td>
 </tr>
</table>
<b>3. Log Analytics</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/LogAnalytics/LogAnalytics.py#L1-L27</td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/LogAnalytics/LogAnalytics.png"></td>
 </tr>
</table>
<b>4. Pair RDD</b>
    <table border="0">
 <tr>
    <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/PairRDD/PairRDD.py#L1-L11</td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/PairRDD/PairRDD.png"></td>
 </tr>
</table>
<b>5. Understanding RDDs</b>
    <table border="0">
 <tr>
    <td><b style="font-size:30px" width="20%">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/UnderstandingRDDs/UnderstandingRDDs.py#L1-L40 </td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/UnderstandingRDDs/UnderstandingRDDs.png"></td>
 </tr>
</table>
<b>6. Word Count</b>
    <table border="0">
 <tr>
    <td><b style="font-size:30px">Code</b></td>
    <td><b style="font-size:30px">Output</b></td>
 </tr>
 <tr>
    <td>https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/WordCount/WordCount.py#L1-L8</td>
    <td><img alt="Dark" src="https://github.com/AlifiyulAkyun/Big-Data/blob/e507a8b16e4ddbd20239ec77a4bd7c1642cfe81b/WordCount/Word%20Count.png"></td>
 </tr>
</table>

## Tugas Praktikum 2
<b>1.</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Kode</b></td>
    <td><b style="font-size:30px">Keterangan</b></td>
 </tr>
 <tr>
    <td>sc</td>
    <td>sc berasal dari SparkContext yang sudah tersedia dalam PySpark. Ini adalah titik masuk utama untuk mengakses semua fitur dan fungsionalitas Spark. SparkContext menyediakan koneksi ke cluster Spark dan mengelola komputasi paralel dalam lingkungan tersebut.Dalam Spark, setiap aplikasi terdiri dari satu SparkContext yang memungkinkan komunikasi antara aplikasi dan cluster Spark. SparkContext bertanggung jawab untuk membagi tugas ke berbagai node di dalam cluster, memuat data dari sumber eksternal seperti Hadoop Distributed File System (HDFS), dan menyimpan hasil akhir di dalam HDFS atau sistem penyimpanan lainnya.</td>
 </tr>
 <tr>
    <td>accumulator</td>
    <td>sc.accumulator adalah sebuah fungsi atau metode pada PySpark, sebuah framework untuk pemrosesan data yang berjalan di atas Apache Spark. Fungsi ini digunakan untuk mengakumulasi nilai secara bertahap pada RDD (Resilient Distributed Datasets) yang di proses pada Spark.Dalam PySpark, sc.accumulator dapat digunakan untuk mengumpulkan nilai atau menghitung jumlah pada setiap proses atau partisi dari RDD, sehingga nilai akhir yang dihasilkan akan merepresentasikan nilai keseluruhan dari RDD tersebut. </td>
 </tr>
 <tr>
    <td>parallelize</td>
    <td>paralellize adalah sebuah metode pada framework pemrosesan data Apache Spark yang digunakan untuk mengubah sebuah koleksi data menjadi RDD (Resilient Distributed Datasets) yang dapat diproses secara terdistribusi dan paralel di dalam cluster komputasi yang terdiri dari beberapa node. </td>
 </tr>
 <tr>
    <td>lambda</td>
    <td>Lambda pada Spark digunakan untuk membuat sebuah fungsi anonim yang dapat digunakan dalam proses pemrosesan data yang dilakukan di dalam Apache Spark. Penggunaan fungsi Lambda pada Spark biasanya diintegrasikan dengan fungsi-fungsi lainnya seperti map(), filter(), dan reduce(). Sebagai contoh, kita dapat menggunakan fungsi Lambda untuk mengubah sebuah RDD dengan menerapkan sebuah operasi matematika pada setiap elemen RDD.</td>
 </tr>
 <tr>
    <td>value</td>
    <td>value adalah argumen atau input variabel yang ingin diparsing. Nilai value pada lambda sendiri dapat diganti, misalnya x, q, dan lain sebagainya.</td>
 </tr>
</table>

<b>2.</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Kode</b></td>
    <td><b style="font-size:30px">Keterangan</b></td>
 </tr>
 <tr>
    <td>broadcast</td>
    <td>Broadcast pada Spark digunakan untuk menyebarkan variabel atau objek secara efisien ke setiap node pada cluster komputasi yang menjalankan proses Spark. Broadcast dapat digunakan untuk mengirim data yang dibutuhkan oleh setiap task pada setiap node di cluster, sehingga tidak perlu mengirim data berulang-ulang pada setiap task yang dijalankan. </td>
 </tr>
 <tr>
    <td>list</td>
    <td>List pada Spark dapat digunakan untuk merepresentasikan sekumpulan data yang terdiri dari beberapa elemen, yang nantinya dapat diolah dan diproses dalam bentuk RDD (Resilient Distributed Datasets) atau DataFrame pada Spark.</td>
 </tr>
 <tr>
    <td>range</td>
    <td>Range pada Spark digunakan untuk membuat RDD (Resilient Distributed Datasets) yang berisi urutan bilangan bulat secara teratur dengan jarak tertentu. Range seringkali digunakan untuk menghasilkan data dummy atau data testing dalam proses pemrosesan data pada Spark.</td>
 </tr>
</table>

<b>3.</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Kode</b></td>
    <td><b style="font-size:30px">Keterangan</b></td>
 </tr>
 <tr>
    <td>textFile</td>
    <td>TextFile pada Spark digunakan untuk membaca file teks dari sistem file lokal atau sistem file yang didukung Hadoop, dan mengembalikan RDD (Resilient Distributed Datasets) berisi baris-baris teks dari file tersebut. TextFile juga dapat digunakan untuk menulis RDD ke dalam file teks.</td>
 </tr>
 <tr>
    <td>filter</td>
    <td>Filter pada Spark digunakan untuk memilih elemen dari RDD (Resilient Distributed Datasets) atau DataFrame yang memenuhi suatu kondisi atau predikat tertentu. Filter dapat digunakan untuk memfilter data berdasarkan kriteria tertentu, seperti data dengan nilai tertentu, data yang memenuhi kondisi tertentu, dan lain-lain.</td>
 </tr>
 <tr>
    <td>cache</td>
    <td>Cache pada Spark digunakan untuk menyimpan RDD (Resilient Distributed Datasets) atau DataFrame pada memori, sehingga data dapat diakses secara cepat tanpa perlu membaca ulang data dari sistem file. Cache pada Spark dapat meningkatkan performa pemrosesan data pada Spark, terutama untuk data yang sering digunakan dalam berbagai operasi pemrosesan data.</td>
 </tr>
 <tr>
    <td>count</td>
    <td>Count pada Spark digunakan untuk menghitung jumlah elemen dalam RDD (Resilient Distributed Datasets) atau DataFrame. Count pada Spark mengembalikan nilai bilangan bulat yang merupakan jumlah elemen dalam RDD atau DataFrame.</td>
 </tr>
</table>

<b>4.</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Kode</b></td>
    <td><b style="font-size:30px">Keterangan</b></td>
 </tr>
 <tr>
    <td>map</td>
    <td>Map pada Spark digunakan untuk melakukan transformasi pada setiap elemen RDD (Resilient Distributed Datasets) atau DataFrame dengan menggunakan suatu fungsi tertentu. Map pada Spark menghasilkan RDD atau DataFrame baru yang memiliki ukuran yang sama dengan RDD atau DataFrame awal, namun dengan nilai yang telah diubah sesuai dengan fungsi yang diberikan.</td>
 </tr>
 <tr>
    <td>collect</td>
    <td>Collect pada Spark digunakan untuk mengambil semua elemen RDD (Resilient Distributed Datasets) atau DataFrame ke dalam memori driver program dan mengembalikan nilai tersebut sebagai suatu koleksi data pada Python. Collect pada Spark dapat digunakan untuk mengambil hasil akhir dari transformasi RDD atau DataFrame, atau untuk mengecek nilai-nilai yang terdapat dalam RDD atau DataFrame.</td>
 </tr>
 <tr>
    <td>len</td>
    <td>Len pada Spark tidak digunakan secara khusus pada Spark. Len pada Python digunakan untuk mengembalikan panjang suatu objek, seperti panjang sebuah string, list, atau tuple.</td>
 </tr>
 <tr>
    <td>keys</td>
    <td>Keys pada Spark digunakan untuk mengambil kunci (key) dari setiap elemen dalam sebuah RDD (Resilient Distributed Datasets) atau pair RDD. Keys pada Spark menghasilkan RDD baru yang hanya berisi kunci dari setiap elemen dalam RDD atau pair RDD</td>
 </tr>
 <tr>
    <td>values</td>
    <td>Mengambalikan RDD dengan value dari setiap tupel.</td>
 </tr>
</table>

<b>5.</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Kode</b></td>
    <td><b style="font-size:30px">Keterangan</b></td>
 </tr>
 <tr>
    <td>defaultParallelism</td>
    <td>Level default dari parallelism untuk digunakan saat tidak diberikan oleh pengguna</td>
 </tr>
 <tr>
    <td>getNumPartitions</td>
    <td>Mengembalikan nilai dari partisi di RDD</td>
 </tr>
 <tr>
    <td>mapPartitionsWithIndex</td>
    <td>Mengambalikan RDD baru dengan mengaplikasikan fungsi untuk setiap pastisi di RDD, ketika melakukan tracking index dari partisi original.</td>
 </tr>
 <tr>
    <td>repartition</td>
    <td>Mengembalikan RDD baru yang memiliki partisi persis numPartition.</td>
 </tr>
 <tr>
    <td>coalesce</td>
    <td>Mengembalikan RDD baru yang dikurangi atau direduksi menjadi numPartitions</td>
 </tr>
 <tr>
    <td>toDebugString</td>
    <td>Deskripsi dari RDD dan dependensi rekursifnya untuk proses debugging.</td>
 </tr>
</table>

<b>6.</b>
<table border="0">
 <tr>
    <td><b style="font-size:30px">Kode</b></td>
    <td><b style="font-size:30px">Keterangan</b></td>
 </tr>
 <tr>
    <td>flatMap</td>
    <td>Mengembalikan RDD baru dengan terlebih dahulu menerapkan fungsi ke semua elem RDD, lalu diratakan hasilnya. </td>
 </tr>
 <tr>
    <td>reduceByKey</td>
    <td>Menggabungkan nilai untuk setiap kunci menggunakan fungsi pengurangan asosiatif dan komutatif.</td>
 </tr>
 <tr>
    <td>split</td>
    <td>Memisahkan string sesuai separator (pemisah) yang didefinisikan.</td>
 </tr>
</table>
